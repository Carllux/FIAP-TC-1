{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO7bTL1r4AyV"
      },
      "source": [
        "Instalando pacotes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação"
      ],
      "metadata": {
        "id": "8L8Iizx2Z73m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependências para scrapping"
      ],
      "metadata": {
        "id": "c0zWKgHvu_3D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3eYeOb_34Yg"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install requests beautifulsoup4 pandas numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web scraping EMBRAPA\n",
        "Utilizando parâmetros de URL e na tabela 'tb_base tb_dados'\n"
      ],
      "metadata": {
        "id": "M1oa-F18FH3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependências para tratamento de dados, caso houvesse maior quantidade de registros, no lugar do levenshtein, utilizar o rapidfuzz"
      ],
      "metadata": {
        "id": "7uiaK3xbvGii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy python-levenshtein unidecode"
      ],
      "metadata": {
        "id": "O4RcKSgcvKa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coleta inicial de dados limpos do site da embrapa para inicio da análise,\n",
        "o script abaixo faz a varredura conjunto a padronização inicial (L = KG, remoção de pontuação de milhas e cabeçalhos, conjunto a adição de contexto de extração)"
      ],
      "metadata": {
        "id": "vT6pYPj-HC6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas necessárias para requisição, parsing HTML, manipulação de dados e controle de arquivos\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------------------- CONFIGURAÇÕES INICIAIS ----------------------\n",
        "\n",
        "# Define o diretório onde os arquivos CSV serão salvos\n",
        "DATA_DIR = \"data/raw\"\n",
        "# Cria o diretório se ele ainda não existir\n",
        "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# URL base do site da Embrapa Vitibrasil\n",
        "base_url = \"http://vitibrasil.cnpuv.embrapa.br/index.php\"\n",
        "# Intervalo de anos para coleta dos dados\n",
        "anos = range(2008, 2024)\n",
        "\n",
        "# Dicionário com as páginas do site a serem raspadas\n",
        "# Cada entrada contém: nome do arquivo, se o ano é necessário e subopções da página (se houver)\n",
        "config_paginas = {\n",
        "    \"opt_02\": {\n",
        "        \"nome_arquivo\": \"producao\",\n",
        "        \"requer_ano\": True,\n",
        "        \"subopcoes\": [None]\n",
        "    },\n",
        "    \"opt_03\": {\n",
        "        \"nome_arquivo\": \"processamento\",\n",
        "        \"requer_ano\": True,\n",
        "        \"subopcoes\": [f\"subopt_{i:02d}\" for i in range(1, 3)]\n",
        "    },\n",
        "    \"opt_04\": {\n",
        "        \"nome_arquivo\": \"comercializacao\",\n",
        "        \"requer_ano\": True,\n",
        "        \"subopcoes\": [None]\n",
        "    },\n",
        "    \"opt_05\": {\n",
        "        \"nome_arquivo\": \"importacao\",\n",
        "        \"requer_ano\": True,\n",
        "        \"subopcoes\": [f\"subopt_{i:02d}\" for i in range(1, 3)]\n",
        "    },\n",
        "    \"opt_06\": {\n",
        "        \"nome_arquivo\": \"exportacao\",\n",
        "        \"requer_ano\": True,\n",
        "        \"subopcoes\": [f\"subopt_{i:02d}\" for i in range(3, 5)]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ---------------------- FUNÇÕES PRINCIPAIS ----------------------\n",
        "\n",
        "def scrape_tabelas(base_url, params):\n",
        "    \"\"\"\n",
        "    Realiza requisição HTTP ao site com os parâmetros fornecidos\n",
        "    e retorna as tabelas HTML encontradas com a classe 'tb_base tb_dados'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        tabelas = soup.find_all('table', {'class': 'tb_base tb_dados'})\n",
        "        return tabelas, soup\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao acessar {params}: {str(e)}\")\n",
        "        return [], None\n",
        "\n",
        "def processar_tabela_com_itens(soup, tabela):\n",
        "    \"\"\"\n",
        "    Processa tabelas que possuem estrutura hierárquica com 'item' e 'subitem'\n",
        "    Retorna um DataFrame estruturado mantendo a relação entre eles\n",
        "    \"\"\"\n",
        "    linhas = tabela.find_all('tr')\n",
        "    dados = []\n",
        "    current_item = None\n",
        "\n",
        "    for linha in linhas:\n",
        "        item = linha.find('td', {'class': 'tb_item'})\n",
        "        if item:\n",
        "            current_item = item.get_text(strip=True)\n",
        "            continue\n",
        "\n",
        "        subitem = linha.find('td', {'class': 'tb_subitem'})\n",
        "        if subitem and current_item:\n",
        "            celulas = [td.get_text(strip=True) for td in linha.find_all('td')]\n",
        "            dados.append([current_item] + celulas)\n",
        "\n",
        "    if dados:\n",
        "        num_colunas = max(len(linha) for linha in dados)\n",
        "        colunas = ['Item'] + [f'Coluna_{i}' for i in range(1, num_colunas)]\n",
        "        df = pd.DataFrame(dados, columns=colunas)\n",
        "    else:\n",
        "        # Caso a tabela não siga o padrão esperado, usa leitura direta\n",
        "        df = pd.read_html(str(tabela))[0]\n",
        "\n",
        "    return df\n",
        "\n",
        "def tratar_valores_nulos(df):\n",
        "    \"\"\"\n",
        "    Substitui valores inválidos ('-', 'nd') por 0 e tenta converter colunas para numérico\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].replace('-', '0')\n",
        "            df[col] = df[col].replace('nd', '0')\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col], errors='ignore')\n",
        "            except:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "def arquivo_existe(nome_arquivo):\n",
        "    \"\"\"\n",
        "    Verifica se o arquivo CSV já existe no diretório de saída\n",
        "    \"\"\"\n",
        "    caminho_completo = os.path.join(DATA_DIR, nome_arquivo)\n",
        "    return os.path.exists(caminho_completo)\n",
        "\n",
        "# ---------------------- FLUXO PRINCIPAL ----------------------\n",
        "\n",
        "# Itera sobre as páginas configuradas\n",
        "for opcao, config in config_paginas.items():\n",
        "    # Gera o nome do arquivo de saída baseado no nome e no intervalo de anos\n",
        "    if config[\"requer_ano\"]:\n",
        "        nome_arquivo = f\"{config['nome_arquivo']}_{min(anos)}_{max(anos)}.csv\"\n",
        "    else:\n",
        "        nome_arquivo = f\"{config['nome_arquivo']}.csv\"\n",
        "\n",
        "    # Pula a coleta se o arquivo já foi salvo anteriormente\n",
        "    if arquivo_existe(nome_arquivo):\n",
        "        print(f\"Arquivo {nome_arquivo} já existe. Pulando coleta...\")\n",
        "        continue\n",
        "\n",
        "    dfs = []  # Lista para armazenar DataFrames por ano/subopção\n",
        "    intervalos = anos if config[\"requer_ano\"] else [None]\n",
        "\n",
        "    # Itera sobre os anos (se aplicável) e subopções da página\n",
        "    for ano in intervalos:\n",
        "        for subopcao in config[\"subopcoes\"]:\n",
        "            params = {\"opcao\": opcao}\n",
        "            if ano is not None:\n",
        "                params[\"ano\"] = ano\n",
        "            if subopcao:\n",
        "                params[\"subopcao\"] = subopcao\n",
        "\n",
        "            print(f\"Coletando: opcao={opcao}, ano={ano}, subopcao={subopcao}\")\n",
        "            tabelas, soup = scrape_tabelas(base_url, params)\n",
        "\n",
        "            for i, tabela in enumerate(tabelas):\n",
        "                try:\n",
        "                    # Decide como processar a tabela com base na estrutura do HTML\n",
        "                    if soup and (soup.find('td', {'class': 'tb_item'}) or tabela.find('td', {'class': 'tb_item'})):\n",
        "                        df = processar_tabela_com_itens(soup, tabela)\n",
        "                    else:\n",
        "                        df = pd.read_html(str(tabela))[0]\n",
        "\n",
        "                    # Limpeza e tratamento de dados nulos\n",
        "                    df = tratar_valores_nulos(df)\n",
        "\n",
        "                    # Adiciona coluna do ano (se aplicável)\n",
        "                    if ano is not None:\n",
        "                        df['Ano'] = ano\n",
        "\n",
        "                    dfs.append(df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar tabela: {str(e)}\")\n",
        "\n",
        "            sleep(2)  # Pausa entre requisições para respeitar o servidor\n",
        "\n",
        "    # Salva o arquivo CSV final se houve coleta\n",
        "    if dfs:\n",
        "        caminho_completo = os.path.join(DATA_DIR, nome_arquivo)\n",
        "        df_final = pd.concat(dfs, ignore_index=True)\n",
        "        df_final.to_csv(caminho_completo, index=False)\n",
        "        print(f\"Dados salvos em {caminho_completo}\")\n",
        "\n",
        "print(\"Coleta concluída!\")\n"
      ],
      "metadata": {
        "id": "69vQ2dNEFM0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MkRqzABmIpMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataframe para padronização e conversão da coluna em lista para a aplicação do fuzzywuzzy"
      ],
      "metadata": {
        "id": "D32L-9T6CsgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV contendo nome de países baseados no IBGE\n",
        "paises_padronizados = pd.read_csv('https://balanca.economia.gov.br/balanca/bd/tabelas/PAIS.csv', encoding='latin1', sep=';').rename(columns={'NO_PAIS': 'Pais'})\n",
        "\n",
        "# Extrair lista de nomes padronizados\n",
        "paises = paises_padronizados['Pais'].tolist()"
      ],
      "metadata": {
        "id": "grv0nDmiYH0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV contendo nome de países baseados no IBGE\n",
        "paises_padronizados_2 = pd.read_csv('http://raw.githubusercontent.com/AndiDittrich/GeoIP-Country-Lists/refs/heads/master/GeoLite2/GeoLite2-Country-CSV_20150407/GeoLite2-Country-Locations-pt-BR.csv', encoding='UTF-8', sep=',').rename(columns={'country_name': 'Pais'})\n",
        "# paises_padronizados_2\n",
        "# Extrair lista de nomes padronizados\n",
        "paises_2 = paises_padronizados_2['Pais'].tolist()"
      ],
      "metadata": {
        "id": "84m0_bbP_Rq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementações do fuzzywuzzy"
      ],
      "metadata": {
        "id": "k-tCxSDXC4uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando FuzzyWuzzy para preparar a base para futuras inclusões de bases de dados de outros países"
      ],
      "metadata": {
        "id": "mI_kzoTfqqVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unidecode\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "def normalizar(texto):\n",
        "    \"\"\"Remove acentos, coloca em minúsculo e tira espaços extras.\"\"\"\n",
        "    if pd.isnull(texto):\n",
        "        return \"\"\n",
        "    return unidecode.unidecode(texto.lower().strip())\n",
        "\n",
        "def fuzzy_match_dataframe(\n",
        "    df_origem,\n",
        "    referencia,\n",
        "    coluna_origem='Pais',\n",
        "    coluna_referencia='Pais',\n",
        "    threshold=85,\n",
        "    scorer=fuzz.token_sort_ratio,\n",
        "    correcoes_manuais=None,\n",
        "    excluir_valores=None  # lista de valores a ignorar\n",
        "):\n",
        "    \"\"\"\n",
        "    Faz fuzzy matching entre países com suporte a:\n",
        "    - Correções manuais via dicionário\n",
        "    - Lista de valores a excluir do matching\n",
        "    \"\"\"\n",
        "\n",
        "    # Normaliza lista de exclusão\n",
        "    excluir_norm = set()\n",
        "    if excluir_valores:\n",
        "        excluir_norm = set(normalizar(v) for v in excluir_valores)\n",
        "\n",
        "    # Verifica referência\n",
        "    if isinstance(referencia, pd.DataFrame):\n",
        "        lista_ref = referencia[coluna_referencia].dropna().tolist()\n",
        "    else:\n",
        "        lista_ref = referencia\n",
        "\n",
        "    lista_ref_norm = [normalizar(p) for p in lista_ref]\n",
        "    ref_dict = dict(zip(lista_ref_norm, lista_ref))  # Para recuperar forma original\n",
        "\n",
        "    correcoes_norm = {}\n",
        "    if correcoes_manuais:\n",
        "        correcoes_norm = {\n",
        "            normalizar(k): v for k, v in correcoes_manuais.items()\n",
        "        }\n",
        "\n",
        "    def corrigir(pais):\n",
        "        pais_norm = normalizar(pais)\n",
        "\n",
        "        # Ignora se estiver na lista de exclusão\n",
        "        if pais_norm in excluir_norm:\n",
        "            return pd.Series([None, None])\n",
        "\n",
        "        # Substituição direta se houver\n",
        "        if pais_norm in correcoes_norm:\n",
        "            return pd.Series([correcoes_norm[pais_norm], 100])\n",
        "\n",
        "        # Aplica fuzzy\n",
        "        match_norm, score = process.extractOne(pais_norm, lista_ref_norm, scorer=scorer)\n",
        "        match_final = ref_dict.get(match_norm) if score >= threshold else None\n",
        "        return pd.Series([match_final, score])\n",
        "\n",
        "    # Aplica correções\n",
        "    df_origem[['País_corrigido', 'threshold']] = df_origem[coluna_origem].apply(corrigir)\n",
        "\n",
        "    # Retorna apenas registros com correspondência válida\n",
        "    return df_origem[df_origem['País_corrigido'].notnull()].copy()\n"
      ],
      "metadata": {
        "id": "wob8wY86_Na0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nomes onde algorítmo sofreu algum problema para funcionar, devido as\n",
        "características dos registros originais, esses sendo filtrados e trabalhados\n",
        "manualmente"
      ],
      "metadata": {
        "id": "7hEp3RDmqCMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correcoes = {\n",
        "    \"Alemanha, República Democrática\": \"Alemanha\", \"Eslovaca, Republica\": 'Eslováquia', 'Países Baixos': 'Holanda'\n",
        "}\n",
        "\n",
        "excluir = [\n",
        "    \"Total\", \"Outros(1)\"\n",
        "]\n",
        "\n",
        "resultado = fuzzy_match_dataframe(\n",
        "    exportacao,\n",
        "    paises_2,\n",
        "    threshold=50,\n",
        "    correcoes_manuais=correcoes,\n",
        "    excluir_valores=excluir\n",
        ")\n"
      ],
      "metadata": {
        "id": "zM9C3oVdB4Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado\n"
      ],
      "metadata": {
        "id": "6ZBL8I041bwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checagem de consistência, para incrementar o tratamento, se necessário\n",
        "(Como podemos ver Coveite apresenta uma threshold grande e acaba por não atender adequadamente, sendo necessária a inclusão no bloco acima)"
      ],
      "metadata": {
        "id": "EAzq5mIiqeRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado_filtrado = resultado[[\"Pais\", \"País_corrigido\", \"threshold\"]].rename(columns={\"País_corrigido\": \"Pais_fuzzy\"})\n",
        "resultado_filtrado_distinto = resultado_filtrado.drop_duplicates(subset=\"Pais\").sort_values(by=\"threshold\", ascending=True).reset_index(drop=True)\n",
        "resultado_filtrado_distinto"
      ],
      "metadata": {
        "id": "cDyU2U5Qurb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado.to_csv('exportacao_2008_2023_corrigido.csv', index=False)"
      ],
      "metadata": {
        "id": "48DhdVuGBdYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}